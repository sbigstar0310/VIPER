actent: 0.0003
actor:
  act: silu
  fan: avg
  inputs: [deter, stoch]
  layers: 2
  maxstd: 1.0
  minstd: 0.1
  norm: layer
  outnorm: false
  outscale: 1.0
  symlog_inputs: false
  unimix: 0.01
  units: 512
  winit: normal
actor_dist_cont: normal
actor_dist_disc: onehot
actor_grad_cont: backprop
actor_grad_disc: reinforce
actor_opt: {clip: 100.0, eps: 1e-05, lateclip: 0.0, lr: 3e-05, opt: adam, warmup: 0,
  wd: 0.0}
amp_rewards: {disag: 0.0, extr: 1.0}
amp_window: 5
batch_length: 32
batch_size: 8
cont_head:
  act: silu
  dist: binary
  fan: avg
  inputs: [deter, stoch]
  layers: 2
  norm: layer
  outnorm: false
  outscale: 1.0
  units: 512
  winit: normal
critic:
  act: silu
  bins: 255
  dist: symlog_disc
  fan: avg
  inputs: [deter, stoch]
  layers: 2
  norm: layer
  outnorm: false
  outscale: 0.0
  symlog_inputs: false
  units: 512
  winit: normal
critic_opt: {clip: 100.0, eps: 1e-05, lateclip: 0.0, lr: 3e-05, opt: adam, warmup: 0,
  wd: 0.0}
critic_slowreg: logprob
critic_type: vfunction
data_loaders: 8
decoder:
  act: silu
  cnn: resnet
  cnn_blocks: 0
  cnn_depth: 32
  cnn_keys: image
  cnn_sigmoid: false
  fan: avg
  image_dist: mse
  inputs: [deter, stoch]
  minres: 4
  mlp_keys: $^
  mlp_layers: 5
  mlp_units: 1024
  norm: layer
  outscale: 1.0
  resize: stride
  vector_dist: symlog_mse
  winit: normal
density_head:
  act: silu
  bins: 255
  dist: symlog_disc
  fan: avg
  inputs: [deter, stoch]
  layers: 2
  norm: layer
  outnorm: false
  outscale: 0.0
  units: 512
  winit: normal
disag_head:
  act: silu
  dist: mse
  fan: avg
  inputs: [deter, stoch, action]
  layers: 2
  norm: layer
  outscale: 1.0
  units: 512
  winit: normal
disag_models: 8
disag_target: [stoch]
discriminator: {act: silu, cnn: resnet, cnn_blocks: 0, cnn_depth: 32, cnn_keys: .*,
  fan: avg, minres: 4, mlp_layers: 5, mlp_units: 1024, norm: layer, resize: stride,
  symlog_inputs: true, winit: normal}
discriminator_head:
  act: silu
  bins: 255
  dist: symlog_disc
  fan: avg
  inputs: [deter, stoch]
  layers: 2
  norm: layer
  outnorm: false
  outscale: 0.0
  units: 512
  winit: normal
dyn_loss: {free: 1.0, impl: kl}
encoder: {act: silu, cnn: resnet, cnn_blocks: 0, cnn_depth: 32, cnn_keys: image, fan: avg,
  minres: 4, mlp_keys: $^, mlp_layers: 5, mlp_units: 1024, norm: layer, resize: stride,
  symlog_inputs: true, winit: normal}
env:
  atari:
    actions: all
    gray: false
    lives: unused
    noops: 0
    repeat: 4
    resize: opencv
    size: [64, 64]
    sticky: true
  cliport:
    size: [128, 128]
  dmc:
    camera: -1
    repeat: 2
    size: [64, 64]
  dmlab:
    episodic: true
    repeat: 4
    size: [64, 64]
  loconav:
    camera: -1
    repeat: 2
    size: [64, 64]
  minecraft:
    break_speed: 100.0
    size: [64, 64]
  rlbench:
    camera_keys: [image_front, image_wrist, image_overhead]
    restrict_to_box: false
    start_gripper_low: false
    terminate_on_success: false
    use_rotation: false
envs: {amount: 4, checks: false, discretize: 0, length: 0, parallel: process, reset: true,
  restart: true}
eval_dir: ''
expl_behavior: None
expl_opt: {clip: 100.0, eps: 1e-05, lr: 0.0001, opt: adam, warmup: 0, wd: 0.0}
expl_rewards: {disag: 0.1, extr: 1.0}
filter: .*
grad_heads: [decoder, density, cont]
horizon: 333
imag_horizon: 15
imag_unroll: false
jax:
  debug: false
  debug_nans: false
  jit: true
  logical_cpus: 0
  metrics_every: 10
  platform: gpu
  policy_devices: [0]
  prealloc: true
  precision: float16
  reward_model_device: 0
  train_devices: [0]
logdir: /root/VIPER/viper_rl_data/policy_result/dmc_clen16_fskip4/cartpole_balance
loss_scales: {actor: 1.0, cont: 1.0, critic: 1.0, density: 1.0, discriminator_reward: 1.0,
  dyn: 0.5, image: 1.0, mp_amp: 1.0, mp_amp_gp: 1.0, rep: 0.1, reward: 1.0, slowreg: 1.0,
  vector: 1.0}
method: name
model_opt: {clip: 1000.0, eps: 1e-08, lateclip: 0.0, lr: 0.0001, opt: adam, warmup: 0,
  wd: 0.0}
prior_rewards: {density: 1.0, disag: 1.0, extr: 0.0}
reference_dir: /dev/null
rep_loss: {free: 1.0, impl: kl}
replay: uniform_relabel
replay_online: false
replay_size: 1000000.0
retnorm: {decay: 0.99, impl: perc_ema, max: 1.0, perchi: 95.0, perclo: 5.0}
return_lambda: 0.95
reward_head:
  act: silu
  bins: 255
  dist: symlog_disc
  fan: avg
  inputs: [deter, stoch]
  layers: 2
  norm: layer
  outnorm: false
  outscale: 0.0
  units: 512
  winit: normal
reward_model: dmc_clen16_fskip4
reward_model_batch_size: 32
reward_model_compute_joint: true
rssm: {act: silu, action_clip: 1.0, classes: 32, deter: 512, fan: avg, initial: learned,
  norm: layer, stoch: 32, unimix: 0.01, units: 512, unroll: false, winit: normal}
run:
  actor_batch: 32
  actor_host: localhost
  actor_port: '5551'
  actor_threads: 1
  env_replica: -1
  eval_eps: 1
  eval_every: 3000
  eval_fill: 0
  eval_initial: true
  eval_samples: 1
  expl_until: 0
  from_checkpoint: ''
  ipv6: false
  log_every: 300
  log_keys_max: ^log_.*
  log_keys_mean: ^log_.*
  log_keys_sum: ^log_.*
  log_keys_video: [image]
  log_zeros: true
  save_every: 900
  script: train
  steps: 300000
  sync_every: 10
  trace_malloc: false
  train_fill: 0
  train_ratio: 64.0
seed: 0
slow_critic_fraction: 0.02
slow_critic_update: 1
task: dmc_cartpole_balance
task_behavior: Prior
uniform_relabel_add_mode: chunk
wrapper: {checks: false, density: true, discretize: 0, length: 0, reset: true}
